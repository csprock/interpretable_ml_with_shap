{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning with SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ydata_profiling import ProfileReport\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "CACHE = True\n",
    "PROFILE = False\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shap.datasets.california()\n",
    "\n",
    "# combined dataframe\n",
    "df = pd.DataFrame(X, columns=X.columns)\n",
    "df['price'] = y\n",
    "\n",
    "with open(\"california_housing_descr.txt\") as f:\n",
    "    print(f.read())\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "if PROFILE:\n",
    "    profile = ProfileReport(df, title=\"California Housing Dataset Profile\", minimal=False)\n",
    "    profile.to_file(\"california_housing_report.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Motivating Example: Linear Regression\n",
    "\n",
    "Linear regression models are simply enough to be considered \"intrinsically interpretable\". The prediction from a linear model is simply the sum of the products of model coefficients and the variable values. \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k\n",
    "$$\n",
    "\n",
    "It might be tempting to use the magnitudes of the coefficients to determine the most important variables, but this would provide a misleading understanding of the model if the units of measure of the variables are different. \n",
    "\n",
    "In the California housing dataset, median income is measured in tens of thousands of dollars and the coefficient is 0.54, which is a similar magnitude to the rest of the coefficients. However, if we measure median income in dollars, then coefficient would 5 orders of magnitude greater than the rest of the coefficients and would be deemed the \"most important\" by magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a linear regression model using the specified features\n",
    "linear_model_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveOccup', 'Population', 'AveBedrms']\n",
    "X_train_selected = X_train[linear_model_features]\n",
    "X_test_selected = X_test[linear_model_features]\n",
    "\n",
    "# Initialize and train the linear regression model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train_selected, y_train)\n",
    "\n",
    "# Print the model coefficients\n",
    "coefficients = pd.DataFrame({\n",
    "    'Feature': linear_model_features,\n",
    "    'Coefficient': linear_model.coef_\n",
    "})\n",
    "print(\"Linear Regression Coefficients:\")\n",
    "print(coefficients)\n",
    "print(f\"\\nIntercept: {linear_model.intercept_:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to view the importance of each variable for an *individual prediction*. Since the predicted value of a linear model is a linear sum of coefficients multipled by the value of the variable (the $\\beta_i  x_i$ terms in the sum), we can decompose the predictions into these products and view their magnitudes since they are all in the same unit of measure as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate predictions manually by multiplying features by coefficients and adding intercept\n",
    "manual_predictions = np.dot(X_test_selected, linear_model.coef_) + linear_model.intercept_\n",
    "np.allclose(manual_predictions, linear_model.predict(X_test_selected))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of the products\n",
    "linear_components = pd.DataFrame(np.concatenate(\n",
    "    [\n",
    "        np.repeat(linear_model.intercept_, X_test_selected.shape[0]).reshape((-1, 1)), \n",
    "        np.multiply(X_test_selected.values, linear_model.coef_)\n",
    "    ], \n",
    "axis = 1), \n",
    "    columns = ['Intercept'] + linear_model_features\n",
    ")\n",
    "\n",
    "np.allclose(linear_components.sum(axis=1), (manual_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these products as feature importances to create some simple visualizations such as a waterfall plot that shows how to get from the intercept of the model to the predicted value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_linear_prediction_waterfall\n",
    "plot_linear_prediction_waterfall(3, linear_components=linear_components,model=linear_model, X_test_selected=X_test_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Explainability for Tree-Based Models\n",
    "\n",
    "We just saw how linear models are easily interpreted and how you can get a nice, clean, preduction \n",
    "\n",
    "In modern applications, the best results are often obtained using large datasets and complex, non-linear models whose output is difficult to interpret and whose inner mechanics are non-transparent and difficult to understand. In many settings, tree-based methods such as random forests and gradient boosting machines achieve the best performance, especially on tabular data that is often found in the business world. \n",
    "\n",
    "(insert shap citation) identify three desirable properties that a good feature importance measure should have\n",
    "\n",
    "1. Consistency: If the model changes in such a way as to increase the marginal importance of a feature, the feature importance for that feature should not decrease\n",
    "2. Prediction-level explainations: the feature importance measure can explain individual predictions, not just the global importance for the entire model\n",
    "3. Local Accuracy (Additivity): the sum of the feature importance measures for an individual prediction sum to the predicted value, i.e. $f(x) = \\sum_{i} \\phi_i$ where the $\\phi_i$ are the feature importance measures for the $i$ th feature. (Note that this requires prediction-level explainations)\n",
    "\n",
    "\n",
    "There are numerous feature importance measures for use with both trees and other classes of models. The ones that are the most often used are:\n",
    "\n",
    "1. Permutation Importance \n",
    "2. Gain\n",
    "\n",
    "We will give an example of each and explain why none of these satisfy all three properties and then introduce SHAP as the only feature importance measure that satisfied all three. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import json\n",
    "\n",
    "\n",
    "if CACHE:\n",
    "\n",
    "    # Load pre-computed best parameters from JSON file\n",
    "    try:\n",
    "        with open('rf_best_params.json', 'r') as f:\n",
    "            best_params = json.load(f)\n",
    "            print(\"Loaded best parameters from file:\", best_params)\n",
    "        \n",
    "        # Create RandomForest model with best parameters\n",
    "        best_rf = RandomForestRegressor(**best_params, random_state=42)\n",
    "        \n",
    "        # Train the model\n",
    "        best_rf.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        test_score = best_rf.score(X_test, y_test)\n",
    "        print(f\"RÂ² Score on Test Set: {test_score:.4f}\")\n",
    "        \n",
    "        # Calculate predictions and MSE on test set\n",
    "        y_pred = best_rf.predict(X_test)\n",
    "        mse = mean_squared_error(y_test, y_pred)\n",
    "        print(f\"MSE on Test Set: {mse:.4f}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Cache file not found, will train model from scratch\")\n",
    "\n",
    "else:  \n",
    "\n",
    "    # Set up hyperparameter search for Random Forest Regressor\n",
    "    # Define the parameter grid to search\n",
    "    param_dist = {\n",
    "        'n_estimators': [50, 100, 150, 200],\n",
    "        'max_depth': [10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2'],\n",
    "        'bootstrap': [True, False]\n",
    "    }\n",
    "\n",
    "    # Create the random forest regressor\n",
    "    rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "    # Set up k-fold cross-validation\n",
    "    cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    # Create the RandomizedSearchCV\n",
    "    rf_random = RandomizedSearchCV(\n",
    "        estimator=rf,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,  # Number of parameter settings to try\n",
    "        scoring='neg_mean_squared_error',\n",
    "        cv=cv,\n",
    "        verbose=1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1  # Use all available cores\n",
    "    )\n",
    "\n",
    "    # Fit the random search\n",
    "    rf_random.fit(X_train, y_train)\n",
    "\n",
    "    # Print the best parameters and score\n",
    "    print(\"Best Parameters:\", rf_random.best_params_)\n",
    "    print(\"Best CV Score:\", -rf_random.best_score_)  # Convert back to positive MSE\n",
    "\n",
    "    # Save best parameters to JSON file\n",
    "\n",
    "    best_params = rf_random.best_params_\n",
    "    with open('rf_best_params.json', 'w') as f:\n",
    "        json.dump(best_params, f, indent=4)\n",
    "\n",
    "    print(\"Best parameters saved to rf_best_params.json\")\n",
    "\n",
    "    # Evaluate on test set\n",
    "    best_rf = rf_random.best_estimator_\n",
    "    test_score = best_rf.score(X_test, y_test)\n",
    "    print(f\"RÂ² Score on Test Set: {test_score:.4f}\")\n",
    "\n",
    "    # Calculate MSE on test set\n",
    "    y_pred = best_rf.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    print(f\"MSE on Test Set: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permutation Importance\n",
    "\n",
    "Permutation importance works by randomly shuffing a feature then watching how accuracy of the model degrades. By breaking the dependence between the feature and the target variable, the idea is that we can see how much the model truely relies on that feature.\n",
    "\n",
    "Since permutation importances is measured with respect to model performance, computing it using the training set can provide misleading results on overfitted models, so it is best practice to calculate it using data not used to train the model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Calculate permutation importance for RandomForest model\n",
    "\n",
    "# Calculate permutation importance\n",
    "perm_importance = permutation_importance(best_rf, X_test, y_test, n_repeats=10, random_state=42)\n",
    "\n",
    "# Sort the permutation importance by value\n",
    "sorted_indices = perm_importance.importances_mean.argsort()\n",
    "sorted_importances = perm_importance.importances_mean[sorted_indices]\n",
    "sorted_features = X.columns[sorted_indices]\n",
    "sorted_std = perm_importance.importances_std[sorted_indices]\n",
    "\n",
    "# Create horizontal bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(range(len(sorted_features)), sorted_importances, xerr=sorted_std, \n",
    "         color='skyblue', edgecolor='navy', alpha=0.7)\n",
    "\n",
    "# Add feature names as y-tick labels\n",
    "plt.yticks(range(len(sorted_features)), sorted_features)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Permutation Importance (Mean Decrease in Model Performance)')\n",
    "plt.title('Feature Importance based on Permutation Importance')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the actual values for reference\n",
    "for feature, importance, std in zip(sorted_features, sorted_importances, sorted_std):\n",
    "    print(f\"{feature}: {importance:.4f} Â± {std:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages\n",
    "\n",
    "* Easy to understand and implement; provides a quick, global overview of feature importance\n",
    "* Can be used with any model\n",
    "* Satisfies the *consistency* property\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Permuting the values of a single features can produce data points outside the distribution of the data when features are correlated. \n",
    "* Does not provide prediction-level feature importances and so cannot be locally accurate\n",
    "* Permutation importance can split the importance between correlated features, making one (or both) features seem less important than they actually are\n",
    "* Permutation importance uses the model *performance* instead of *output*, and this may not be what you want depending on the context\n",
    "\n",
    "The plot below uses some dummy data to illustrate the effect on the data disbritution of permuting a feature when two features are correlated. The points in red are the result of permuting Feature 1, which generates unrealistic data points that are outside the joint distribution of Features 1 and 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_permuted_correlated_features\n",
    "plot_permuted_correlated_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gain \n",
    "\n",
    "Gain a feature importance measure that is unique to tree-based models that is calculated as the total reduction of loss or impurity contributed by all splits\n",
    "for a given feature. \n",
    "\n",
    "### Advantages\n",
    "\n",
    "* Is calculated \"for free\" with the training of the model\n",
    "\n",
    "\n",
    "### Disadvantages\n",
    "\n",
    "* Since it is based on the *training data*, it is suseptible to overfitting\n",
    "* Does not satisfy any of the three properties of consistency, prediction-level explainations or local accuracy\n",
    "* It favors continues features over categorical features since there are more opprotunities for splitting. This is also true for high-cardinality categorical features as well that have large numbers of possible splits to choose from\n",
    "* Not model agnostic, only works with tree-based methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate gain importance from the random forest model\n",
    "gain_importance = pd.Series(best_rf.feature_importances_, index=X_train.columns).sort_values(ascending=True)\n",
    "\n",
    "# Create horizontal bar plot for gain importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "gain_importance.plot.barh(color='salmon', edgecolor='darkred')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Gain Importance')\n",
    "plt.title('Feature Importance based on Gain (Gini Impurity Reduction)')\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.6)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print the actual values\n",
    "for feature, importance in gain_importance.items():\n",
    "    print(f\"{feature}: {importance:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SHAP Values for Tree-Based Models\n",
    "\n",
    "SHAP values stand for **SHapley Additive Predictions** were introduced in 2017 by Lundberg and Lee [1]. They are based on Shapely values from cooperative game theory, which is a theoretically sound way to fairly allocate the payouts to players in a coopoerative game. We won't take the game theory connections too far here, but you can think of the \"game\" as the machine learning model being explained, the \"players\" as the input features to the model, and the \"payout\" the model predictions. SHAP values calculate the contribution each feature made to the prediction. \n",
    "\n",
    "Lundberg and Lee showed that SHAP values are the only explainatory model that satisfies the three properties that we discussed earlier. \n",
    "\n",
    "1. Consistency\n",
    "2. Local Accuracy / Additivity\n",
    "3. Prediction-level explainations\n",
    "\n",
    "Formally, additivity means that if $\\phi_i$ is the SHAP value for the $i$ feature, the sum of the SHAP values equals *the difference between the model output and the expected output*. \n",
    "\n",
    "$$\n",
    "f(x) = E[f(x)] + \\sum_{i=1}^F \\phi_i\n",
    "$$\n",
    "\n",
    "These properties unlock a variety of rich visualizations and diagnostic plots that we can use in place of the global feature importance measures that we just discussed. They can also be augmented by traditional Partial Dependence Plots and Individual Conditional Expectation plots, both of which we will review later in this presentation. \n",
    "\n",
    "### Some SHAP Theory\n",
    "\n",
    "Formally, let \n",
    "\n",
    "$$\n",
    "\\phi_i =  \\sum_{S\\subseteq F/\\{i\\}} \\frac{1}{|F|}  \\frac{1}{\\binom{|F|-1}{|S|}} \\big[ f_{S\\cup\\{i\\}} (x_{S\\cup\\{i\\}}) - f_S(x_S) \\big]\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "* $|F|$ is the number of features\n",
    "* $|S|$ is the number of features in the subset $S \\subset F$\n",
    "* $f_{S\\cup\\{i\\}}= E[f(x) | x_{S\\cup\\{i\\}}]$ is the conditional expectation of the model given the features $x_{S\\cup\\{i\\}}$\n",
    "* $ f_S = E[f(x) | x_S]$ is the conditional expectation of the model given the features $x_S$\n",
    "\n",
    "\n",
    "$\\phi_0 = f_\\empty(\\empty)$ \n",
    "\n",
    "**In Words**\n",
    "\n",
    "A one-line definition:\n",
    "\n",
    "> A SHAP value is the average marginal change in the model output from adding a feature to a subset of features, averaged over all such subsets not containing that feature. \n",
    "\n",
    "More precisely: \n",
    "\n",
    ">To calculate the SHAP value for feature $i$, we consider all possible subsets $S$ of features that exclude feature $i$. For each subset, we compute the marginal contribution of adding feature $i$ to that subset, which is the difference between the expected model output when $S$ and feature $i$ are known versus when only $S$ is known. We then take the weighted average of these contributions across all possible subsets where the weights are related to the number of such subsets. \n",
    "\n",
    "In steps: \n",
    "\n",
    "1. Train a machine learning model $f$\n",
    "2. For each feature $i$, consider all the subsets that exclude $i$\n",
    "3. Compute the expected difference in the expected model outputs $E[f(x) | x_{S\\cup\\{i\\}}] - E[f(x) | x_S]$ with and without the feature\n",
    "4. Average over all subsets, with weights equal to the probability of selecting that particular subset\n",
    "\n",
    "Let's unpack the last of these. \n",
    "\n",
    "##### The Conditional Expectation\n",
    "\n",
    "What does $E[f(x) | x_S]$ mean? \n",
    "\n",
    "> It is the model's average prediction when you keep the chosen features in $S$ fixed at their values for a particular data point and average over the other features.  \n",
    "\n",
    "In SHAP, we are setting aside the features $i$ and the features in $S$, holding their values constant for a particular data point, then averaging over the remaining features with and with out $i$ and then computing the difference to see the effect of adding $i$ has on the model output. \n",
    "\n",
    "##### The Weights \n",
    "The weights in the sum above are the probability of selecting a particular subset. The term has two parts: \n",
    "\n",
    "Given a feature $i$, the number of subsets of $S\\subseteq F / \\{i \\}$ is\n",
    "$$\n",
    "\\binom{|F|-1}{|S|}\n",
    "$$, \n",
    "\n",
    "so the probability of selecting a subset, conditional on $i$ (and assuming selection happens uniformly) is\n",
    "\n",
    "$$\n",
    " \\frac{1}{\\binom{|F|-1}{|S|}} \n",
    "$$\n",
    "\n",
    "Since there are $|F|$ features in the model, the probability of selecting one of them uniformly is $\\frac{1}{|F|}$. So the joint probability of selecting feature $i$ and subset $S\\subseteq F / \\{i \\}$ is therefore\n",
    "\n",
    "$$\n",
    "\\frac{1}{|F|}  \\frac{1}{\\binom{|F|-1}{|S|}} \n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "\n",
    "F = len(X.columns)\n",
    "\n",
    "combinations = {}\n",
    "for i in range(1, F):\n",
    "    combinations[i] = comb(F, i)\n",
    "\n",
    "# Create a DataFrame to store the combinations\n",
    "combinations_df = pd.DataFrame.from_dict(combinations, orient='index', columns=['Combinations'])\n",
    "combinations_df.reset_index(inplace=True)\n",
    "\n",
    "total_combinations = combinations_df['Combinations'].sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the number of combinations by subset size\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(combinations_df['index'], combinations_df['Combinations'], color='skyblue', edgecolor='navy')\n",
    "\n",
    "# Adding a curve to highlight the pattern\n",
    "plt.plot(combinations_df['index'], combinations_df['Combinations'], 'ro-', linewidth=2)\n",
    "\n",
    "# Add annotations for each point\n",
    "for i, row in combinations_df.iterrows():\n",
    "    plt.annotate(f\"{int(row['Combinations'])}\", \n",
    "                 (row['index'], row['Combinations']),\n",
    "                 textcoords=\"offset points\", \n",
    "                 xytext=(0,10), \n",
    "                 ha='center')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Size of Subset, |S|', fontsize=12)\n",
    "plt.ylabel('Number of Combinations', fontsize=12)\n",
    "plt.title(f'Number of Combinations for Subsets of |F| - 1 = 8 Features', fontsize=14)\n",
    "\n",
    "# Format x-axis\n",
    "plt.xticks(combinations_df['index'])\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add a note about the total\n",
    "plt.figtext(0.5, 0.01, f\"Total number of combinations: {total_combinations}\", \n",
    "            ha=\"center\", fontsize=12, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the number of possible combinations explodes exponentially with the number of features. The number of subsets in $F/\\{i\\}$ is 254, and since there are 9 features, the number of subsets to evaluate is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(X.columns) * total_combinations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is a lot of subsets! As you can see, the number of subsets explodes exponentially with the number of features.\n",
    "\n",
    "\n",
    "### Computational Difficulties and TreeSHAP\n",
    "\n",
    "There are two main issues with SHAP values:\n",
    "\n",
    "1. Estimating the conditional expectations $E[f(x) | x_S]$ efficiently\n",
    "2. The combinatorical complexity of the SHAP value equation\n",
    "\n",
    "Fortunately, the breakthrough that Lundberg, Erion and Lee made in 2019 [2]  discovering a fast algorithm for computing SHAP values for tree-based models. The model is polynomial time, and allows large models with many features on large dataset to be explained quickly using SHAP. The algorithm is called TreeSHAP. \n",
    "\n",
    "The algorithm is able to compute  $E[f(x) | x_S]$ and does not require sampling or assuming features are independent.\n",
    "\n",
    "# Visualizations and Applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from shap import TreeExplainer\n",
    "\n",
    "\n",
    "X_test_sample = X_test.iloc[:200]\n",
    "\n",
    "\n",
    "# Initialize the TreeExplainer\n",
    "explainer = TreeExplainer(model = best_rf, feature_perturbation=\"tree_path_dependent\")\n",
    "shap_values = explainer(X_test_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now verify the **local accuracy** property and confirm that the sum of the SHAP values equals the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(best_rf.predict(X_test_sample), shap_values.values.sum(axis=1) + explainer.expected_value[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set some example points\n",
    "east_oakland_idx = 5\n",
    "san_francisco_idx = 0\n",
    "rancho_cucamonga_idx = 1\n",
    "visalia_idx = 33\n",
    "ukaiah_idx = 34"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Prediction Plots\n",
    "\n",
    "### Waterfall Plot\n",
    "\n",
    "SHAP provides plots that break down individual predictions into feature contributions (the SHAP values). The waterfall plot is a good way to see which features contributed to the prediction, their magnatude and direction. Blue arrows are for features that negatively contribute to the prediction relative to the baseline and the red arrows are for features that positively contribute relative to the baseline value. \n",
    "\n",
    "Remember that the SHAP values and arrows are *relative to the baseline value*.\n",
    "\n",
    "This plot also offers a nice way to visualize the *local accuracy* property. We can see that the waterfall plot starts at the expected value $E[f(x)]$, which is the predicted value of the model with no features present, and then sum to the predicted value, which is when all features are present. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_idx = san_francisco_idx\n",
    "shap.plots.waterfall(shap_values[sample_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import map_point\n",
    "\n",
    "\n",
    "# sanity-check the interpretation of the SHAP plot by looking at where in California the sample is located\n",
    "map_point(X_test_sample.iloc[sample_idx]['Latitude'], X_test_sample.iloc[sample_idx]['Longitude'], \n",
    "          y_test[sample_idx], y_pred[sample_idx], X_test_sample.iloc[sample_idx]['MedInc'], \n",
    "          X_test_sample.iloc[sample_idx]['HouseAge'], X_test_sample.iloc[sample_idx]['AveRooms'], \n",
    "          X_test_sample.iloc[sample_idx]['AveBedrms'], X_test_sample.iloc[sample_idx]['Population'], \n",
    "          X_test_sample.iloc[sample_idx]['AveOccup'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Force Plots\n",
    "\n",
    "Force plots represent a more compact visualization than the waterfall plot, but shows the same information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a force plot for the selected sample\n",
    "shap.plots.force(shap_values[sample_idx], matplotlib=False, show=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Plots\n",
    "\n",
    "### SHAP Summary Plot\n",
    "\n",
    "SHAP values can be aggregated to create to measure the global feature importance and get a ranking of the most important features, similar to the permutation and gain methods.\n",
    "\n",
    "* Left:  feature importance ranked by the maximum absolute value of the feature's SHAP values\n",
    "* Right: feature importance ranked to the mean absolute value of the feature's SHAP values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "shap.plots.bar(shap_values.abs.max(0), max_display=10, show=False, ax=ax[0], )\n",
    "ax[0].set_title(\"SHAP Feature Importance (Max)\", fontsize=16)\n",
    "\n",
    "\n",
    "shap.plots.bar(shap_values.abs.mean(0), max_display=10, show=False, ax=ax[1], )\n",
    "ax[1].set_title(\"SHAP Feature Importance (Mean)\", fontsize=16)\n",
    "\n",
    "plt.tight_layout(pad=3)\n",
    "#plt.suptitle(\"SHAP Feature Importance Comparison\", fontsize=18, y=1.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beeswarm Plot\n",
    "\n",
    "Beeswarm plots combine information information about the *importance*, *direction* and *distribution* of feature effects.\n",
    "\n",
    "The features are sorted by the mean absolute value of the SHAP values for the feature and each dot displayed is an individual observation. Each dot's position on the x-axis is the SHAP value for that data instance's feature value. Negative values mean that feature had a negative contribution to the predicted output and a positive value means that feature had a positive contribution to the predicted output. \n",
    "\n",
    "The color of each dot is the value of the feature itself, scaled such that red means \"high\" and blue means \"low\". \n",
    "\n",
    "This combination of chart aspects allows us to get a sense of \n",
    "\n",
    "* The *spread* of SHAP values.\n",
    "* The *feature's overall importance*: features with a wider spread and more dots that are far from zero have more global importance and features whose SHAP values cluster near zero have little influence on the model. \n",
    "* The *correlation between the feature value and its impact* via the color pattern: \n",
    "\n",
    "For example, looking at the `MedInc` (median income) feature, we can make several observations:\n",
    "\n",
    "1. Based on the color pattern, lower median incomes tend to have a negative impact on the predicted home price. \n",
    "2. There is wide variation, with many points bunched below zero and a few number of high-income points having a large positive impact on the model. \n",
    "    - This is consistent with a \"Pareto\" wealth/income distribution, with a few areas of concentrated money with high home prices (this is characteristic of the California economy in general, with places like San Francisco, Beverley Hills, Palo Alto, Carmel etc. having very high incomes and home prices and many poor areas further inland from the coast)\n",
    "3. The plot shows an interesting pattern for the geographic features. `Longtitude` is measured in *decimal degrees*, and we see that as we move further West (as measured by more negative values for this feature), the feature has a positive effect on the predicted output. \n",
    "    - This is consistent with the geography of California, where housing is more expensive the closer to the ocean you get. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values, show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SHAP Dependence Plot\n",
    "\n",
    "**SHAP Dependence Plots** are one of the must useful plots that are available to users of SHAP values. They show how the importance of a feature changes with its values. Each point in the scatter plot is an individual observation. The points can be optionally colored by the values of another feature to reveal interactions. \n",
    "\n",
    "* Points around the $y = 0$ line are points for which the feature has little impact on the model.\n",
    "* The plot can reveal trends in feature importance\n",
    "    - monotonic trends indicate the feature has a consistent effect\n",
    "    - otherwise it is indicative of a context-dependent effect, perhaps dependent on other features. \n",
    "* Vertical dispersion of the SHAP values around a feature value indicate interactions with other values, i.e. for a given value of the feature, its effect varies, which must be due to some other factor\n",
    "\n",
    "In the example below, we look at a simple dependence plot for the `MedInc` feature. We can see that as median income increases, it has a stronger positive impact on the model output. From where the points cross the $y=0$ line, we can see that having a block that has a median income below 40K starts to have a negative impact on the prediction. Based on the location of the density histogram, median income has a negative effect on the prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP dependence plot for longitude colored by median income\n",
    "shap_scatter = shap.plots.scatter(shap_values[:, \"MedInc\"], show=False)\n",
    "\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "\n",
    "# Mark the sample points with different markers and colors\n",
    "sample_points = {\n",
    "    'San Francisco': san_francisco_idx,\n",
    "    'East Oakland': east_oakland_idx,\n",
    "    'Rancho Cucamonga': rancho_cucamonga_idx,\n",
    "    'Visalia': visalia_idx,\n",
    "    'Ukaiah': ukaiah_idx\n",
    "}\n",
    "\n",
    "\n",
    "# Add vertical lines and annotations for sample points\n",
    "for name, idx in sample_points.items():\n",
    "    # Extract MedInc value and corresponding SHAP value\n",
    "    medinc = X_test_sample.iloc[idx]['MedInc']\n",
    "    shap_value = shap_values[idx, \"MedInc\"].values\n",
    "\n",
    "    # Mark the point\n",
    "    ax.plot(medinc, shap_value, 'o', markersize=10, \n",
    "            markerfacecolor='none', markeredgecolor='black', markeredgewidth=2)\n",
    "    \n",
    "    # Add label\n",
    "    ax.annotate(name, xy=(medinc, shap_value), xytext=(10, 10),\n",
    "                textcoords='offset points', fontsize=12,\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "\n",
    "ax.axhline(0, color='k', linestyle='--')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "In the example below, we plot the SHAP dependence plot for the `Longtitude` feature and color by the `Latitude` feature. We can immediately observe the following: \n",
    "\n",
    "* There is a trend where points in Western most latitudes and Eastern most latitudes are heavily influenced by their location. \n",
    "    - These represent the coastal areas of the Bay Area and the inland parts of Southern California, respectively, as indicated by the shading where blue is further south and red is further north.\n",
    "* The vertical dispersion between San Francisco and East Oakland shows that the effect of longtitude in and around the SF Bay Area is heavily influenced by other features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SHAP dependence plot for longitude colored by latitude\n",
    "shap_scatter = shap.plots.scatter(shap_values[:, \"Longitude\"], color=shap_values[:, \"Latitude\"], show=False)\n",
    "\n",
    "# Get the current figure and axes\n",
    "fig = plt.gcf()\n",
    "ax = plt.gca()\n",
    "\n",
    "# Mark the sample points with different markers and colors\n",
    "sample_points = {\n",
    "    'San Francisco': san_francisco_idx,\n",
    "    'East Oakland': east_oakland_idx,\n",
    "    'Rancho Cucamonga': rancho_cucamonga_idx,\n",
    "    'Visalia': visalia_idx,\n",
    "    'Ukaiah': ukaiah_idx\n",
    "}\n",
    "\n",
    "# Store SHAP values for the sample points\n",
    "shap_values_at_locations = {}\n",
    "\n",
    "# Add vertical and horizontal lines for each sample point\n",
    "for name, idx in sample_points.items():\n",
    "    # Extract longitude value and corresponding SHAP value for the sample point\n",
    "    longitude = X_test_sample.iloc[idx]['Longitude']\n",
    "    shap_value = shap_values[idx, \"Longitude\"].values\n",
    "    shap_values_at_locations[name] = shap_value\n",
    "    \n",
    "    # Plot cross-hairs for each point\n",
    "    ax.axvline(x=longitude, color='gray', linestyle='--', alpha=0.5)\n",
    "    ax.axhline(y=shap_value, color='gray', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    # Mark the point with a star\n",
    "    ax.plot(longitude, shap_value, 'o', markersize=10, \n",
    "            markerfacecolor='none', markeredgecolor='black', markeredgewidth=2)\n",
    "    \n",
    "    # Add label for the point\n",
    "    ax.annotate(name, xy=(longitude, shap_value), xytext=(10, 10),\n",
    "                textcoords='offset points', fontsize=12,\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0.2'))\n",
    "\n",
    "# add zero line\n",
    "ax.axhline(y=0, color='k', linestyle='--')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using with Partial Dependence Plots\n",
    "\n",
    "**Partial Dependence Plots (PDPs)** complement SHAP values by showing how the model's predicted output changes as a function of a single feature, averaging over all other features. While SHAP values show the importance of features for individual predictions, PDPs provide a global view of how a feature affects predictions across the entire dataset.\n",
    "\n",
    "\n",
    "PDPs illustrate the *marginal effect* of a feature on the predicted outcome by showing the average effect of the feature on the prediction, marginalized over all other features. What this essentially means is varying the value of a single feature while averaging over the remaining features. (Note on causaulity: the effect is specific to the model, and not necessarily causal in reality). \n",
    "\n",
    "#### Differences Between SHAP and PDPs\n",
    "\n",
    "- **SHAP values**: Show contribution of a feature to a specific prediction, accounting for feature interactions\n",
    "- **PDPs**: Show the average effect of a feature across all predictions, averaging out interactions\n",
    "\n",
    "When used together, they provide complementary insights:\n",
    "- PDPs show general trends in how features affect predictions\n",
    "- SHAP values reveal how these effects vary across individual data points and detect feature interactions\n",
    "\n",
    "The longitude PDP in the next cell demonstrates this complementary relationship - showing how the average predicted house price changes from west to east across California, while SHAP values reveal how this effect differs for specific locations (the impact is higher in places like the SF Bay Area and inland Southern California)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import partial_dependence\n",
    "\n",
    "# Calculate partial dependence for longitude\n",
    "pdp_result = partial_dependence(\n",
    "    best_rf, \n",
    "    X_test_sample, \n",
    "    features=['Longitude'], \n",
    "    kind='average',\n",
    "    grid_resolution=50\n",
    ")\n",
    "\n",
    "# Extract PDP values and grid points\n",
    "longitude_grid = pdp_result['grid_values'][0]\n",
    "pdp_values = pdp_result['average'][0]\n",
    "\n",
    "# Create PDP plot\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.plot(longitude_grid, pdp_values, 'k')\n",
    "plt.xlabel('Longitude', fontsize=12)\n",
    "plt.ylabel('Partial Dependence', fontsize=12)\n",
    "plt.title('Partial Dependence Plot for Longitude', fontsize=14)\n",
    "plt.grid(True, linestyle='--', alpha=0.6)\n",
    "\n",
    "# Mark specific locations\n",
    "for name, idx in sample_points.items():\n",
    "    longitude = X_test_sample.iloc[idx]['Longitude']\n",
    "    # Find the closest index in our grid\n",
    "    closest_idx = np.abs(longitude_grid - longitude).argmin()\n",
    "    pd_value = pdp_values[closest_idx]\n",
    "    \n",
    "    plt.plot(longitude, pd_value, 'ro', markersize=8)\n",
    "    plt.annotate(name, (longitude, pd_value), \n",
    "                 xytext=(10, 5), textcoords='offset points',\n",
    "                 arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=.2'))\n",
    "\n",
    "\n",
    "plt.annotate('â West (Coast)', xy=(-123, min(pdp_values)), xytext=(0, -10), \n",
    "             textcoords='offset points', fontsize=10, ha='center')\n",
    "plt.annotate('East (Inland) â', xy=(-117, min(pdp_values)), xytext=(0, -10), \n",
    "             textcoords='offset points', fontsize=10, ha='center')\n",
    "\n",
    "plt.figtext(0.5, 0.01, \n",
    "            \"Compare with SHAP: Partial dependence shows average model response across all longitude values,\\n\"\n",
    "            \"while SHAP values show the feature's impact for specific instances.\", \n",
    "            ha=\"center\", fontsize=10, bbox={\"facecolor\":\"lightgray\", \"alpha\":0.5, \"pad\":5})\n",
    "\n",
    "plt.tight_layout(rect=[0, 0.05, 1, 0.95])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An application of SHAP values is their use in clustering. Here are some facts about the use of SHAP values for clustering. \n",
    "\n",
    "* In regression models, the unit of measure of a SHAP value is the same unit as the target variable, so all SHAP values for all features are measured in the same units. \n",
    "\n",
    "* Clustering using SHAP values also has an interesting interpretation - data in the same cluster had similar feature importances, which is a different interpretation that in regular clustering, which usually groups points based on spatial proximity. \n",
    "\n",
    "* SHAP values with wide dispersion will tend to dominate the clustering, so if you don't want this, you can still standardize the SHAP values first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.heatmap(\n",
    "    shap_values,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "1. Lundberg, S. M., & Lee, S.-I. (2017). *A Unified Approach to Interpreting Model Predictions*. In Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS 2017). [https://arxiv.org/abs/1705.07874](https://arxiv.org/abs/1705.07874)\n",
    "\n",
    "2. Lundberg, S. M., Erion, G. G., & Lee, S.-I. (2019). *Consistent Individualized Feature Attribution for Tree Ensembles*. arXiv preprint arXiv:1802.03888. [https://arxiv.org/abs/1802.03888](https://arxiv.org/abs/1802.03888)\n",
    "\n",
    "3. Frye, C., Rowat, C., & Feige, I. (2020). *Asymmetric Shapley Values: Incorporating Causal Knowledge into Model-Agnostic Explainability*. In Advances in Neural Information Processing Systems (NeurIPS 2020). https://arxiv.org/abs/1910.06358\n",
    "\n",
    "4. SHAP Documentation: https://shap.readthedocs.io\n",
    "\n",
    "5. Molnar, Christoph. *Interpretable Machine Learning: A Guide for Making Black Box Models Explainable*. 3rd ed., 2025. ISBN: 978-3-911578-03-5. Available at: https://christophm.github.io/interpretable-ml-book\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
