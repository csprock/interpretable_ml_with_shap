{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable Machine Learning with SHAP Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /app\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "import shap\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "#selected_features = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup']\n",
    "X, y = shap.datasets.california()\n",
    "#X = X[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Motivating Example: Linear Regression\n",
    "\n",
    "Linear regression models are simply enough to be considered \"intrinsically interpretable\". The prediction from a linear model is simply the sum of the products of model coefficients and the variable values. \n",
    "\n",
    "$$\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\dots + \\beta_k x_k\n",
    "$$\n",
    "\n",
    "It might be tempting to use the magnitudes of the coefficients to determine the most important variables, this would provide a misleading understanding of the model if the units of measure of the variables are different. \n",
    "\n",
    "In the California housing dataset, median income is measured in tens of thousands of dollars and the coefficient is 0.54, which is a similar magnitude to the rest of the coefficients. However, if we measure median income in dollars, then coefficient would 5 orders of magnitude greater than the rest of the coefficients and would be deemed the \"most important\" by magnitude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = LinearRegression(fit_intercept=True)\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "for name, c in zip(X_train.columns, linear_model.coef_.tolist()):\n",
    "    print(f\"{name}:  {c}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a way to view the importance of each variable for an *individual prediction*. Since the predicted value of a linear model is a linear sum of coefficients multipled by the value of the variable (the $\\beta_i  x_i$ terms in the sum), we can decompose the predictions into these products and view their magnitudes since they are all in the same unit of measure as the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify the sum of products equals the model predictions\n",
    "np.allclose(\n",
    "    np.multiply(X_test.values, linear_model.coef_).sum(axis=1) + linear_model.intercept_, # manually calculate predictions using sum of products\n",
    "    linear_model.predict(X_test)                                                          # model predictions\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a matrix of the products\n",
    "linear_components = np.concatenate(\n",
    "    [\n",
    "        np.repeat(linear_model.intercept_, X_test.shape[0]).reshape((-1, 1)), \n",
    "        np.multiply(X_test.values, linear_model.coef_)\n",
    "    ], \n",
    "axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Few Charts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = linear_components[105, :]\n",
    "labels = ['intercept'] + [c for c in X.columns]\n",
    "\n",
    "# Calculate the cumulative values\n",
    "cumulative = np.cumsum(values)\n",
    "cumulative = np.insert(cumulative, 0, 0)  # Insert a starting point of 0 for visual clarity\n",
    "\n",
    "# The positions where each bar starts\n",
    "starts = cumulative[:-1]\n",
    "\n",
    "# Define colors for positive and negative changes\n",
    "colors = ['red' if val >= 0 else 'lightblue' for val in values]\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot each bar\n",
    "for i in range(len(values)):\n",
    "    ax.barh(i, values[i], left=starts[i], color=colors[i], edgecolor='black')\n",
    "\n",
    "# Add labels to each bar for clarity\n",
    "for i in range(len(values)):\n",
    "    ax.text(starts[i] + values[i] / 2, i, f'{round(values[i],2):+}', ha='center', color='black', fontweight='bold')\n",
    "\n",
    "# Configure x-axis with labels\n",
    "ax.set_yticks(range(len(labels)))\n",
    "ax.set_yticklabels(labels, rotation=45, ha='right')\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_ylabel('Variables')\n",
    "ax.set_title('Waterfall Chart for Linear Model Effects')\n",
    "\n",
    "# Add a grid for readability\n",
    "ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "ax.set_xlim((values.min()-0.5, values.max()+0.05))\n",
    "ax.axvline(values.sum(), linestyle='--')\n",
    "ax.axvline(0)\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.partial_dependence_plot(\n",
    "    \"MedInc\",\n",
    "    linear_model.predict,\n",
    "    X_test,\n",
    "    ice=False,\n",
    "    model_expected_value=True,\n",
    "    feature_expected_value=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SHAP Values\n",
    "\n",
    "It would be useful if there were a similar additive decomposition for complex, non-linear models such as neural networks, random forests and boosted trees. Fortunately, there is. SHAP values stand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_estimators = 75,\n",
    "    criterion=\"squared_error\",\n",
    "    max_depth=4,\n",
    "    min_samples_split=10,\n",
    "    max_features=\"sqrt\"\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "shap_model = shap.TreeExplainer(model)\n",
    "shap_values = shap_model(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(shap_values[10, ...])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Definition of a SHAP Value\n",
    "\n",
    "$$\n",
    "\\phi_i = \\sum_{S\\subseteq F/\\{i\\}}  \\binom{|F|-1}{|S|}^{-1} \\big[ f_{S\\cup\\{i\\}} (x_{S\\cup\\{i\\}}) - f_S(x_S) \\big]\n",
    "$$\n",
    "\n",
    "where $\\phi_0 = f_\\empty(\\empty)$\n",
    "\n",
    "In words:\n",
    "\n",
    "\"A SHAP value is the marginal change in the model output obtained by adding the $i$ th feature, conditioned on the set of features $S$ already being in the model, averaged over all subsets $S \\subset F$ not containing $i$ \""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.force(shap_values[10, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.beeswarm(shap_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that the sum of the SHAP values equals the model predictions\n",
    "np.allclose(np.sum(shap_values.values, axis=1) + shap_model.expected_value, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"MedInc\", shap_values.values, X_test, interaction_index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.partial_dependence_plot(\"MedInc\", model.predict, X_test, ice=False, model_expected_value=True, feature_expected_value=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can obtain \"global\" feature importances using SHAP values by averaging the magnitude of the SHAP values for each feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.bar(shap_values)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
